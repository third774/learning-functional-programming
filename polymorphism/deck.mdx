export { default as theme } from "../theme/theme";

# 𝝺

# Polymorphism & Abstraction

```notes
Today's talk is going to be on polymorphism and abstraction, how it differs in FP from OO, and what kinds of tradeoffs we make when using abstractions vs concrete types.
```

---

> Polymorphism is the provision of a single interface to entities of different types or the use of a single symbol to represent multiple different types.

— wikipedia

```notes
Polymorphic types are called generics in many languages like Java and C#. They're called templates in C++

In object oriented programming, these are used heavily with data structures and classes that inherit from other classes and not so much with functions.

This is where functional programming will depart greatly from other statically typed languages.

Languages like Elm, Haskell, Purscript, F♯ rely heavily on functions that are polymorphic meaning functions that can operate on more than just a single concrete type
```

---

# Parametric Polymorphism

```haskell
concreteParam :: Int -> Int
genericParam :: a -> a
```

```notes
Parametric polymorphism is when a function has a parameter, and instead of it being of a fixed type, it's polymorphic. This is distinct from polymorphism via inheritance.

So when we talk about the thing that is polymorphic (many shapes) - the idea is - you can write a function or create a data structure that works with a type where you don't know what it is.

It sounds crazy, but you're going to take in a parameter and have no idea what it's gonna be, and do something useful with that.
```

---

# Constraints

(TypeScript example)

```typescript
interface HasFoo {
  foo: string;
}

function logFoo<T extends HasFoo>(somethingThatHasFoo: T) {
  console.log(somethingThatHasFoo.foo);
}

logFoo({ foo: "heyo" }); // valid
logFoo({ bar: false }); // invalid
```

```notes
Constraints - most languages that support generic types allow you to constrain what types are allowed to be used for that generic.

In OO, these constraints usually have to do with the shape of classes - like what properties and methods are available on them?

In FP, these are usually TypeClasses - like, if it's a number, we know we can perform basic arithmatic operations on it. If it's orderable, we know we can sort it.

Typeclasses are the closest thing FP has to inheritance, and we might touch on those later.

The point is - there can be a sort of spectrum here.
* We have types for which we know everything - like a string.
* And types for which we know nothing - like a fully polymorphic type.
* And we have things in between. Things that implement a certain interface - or a concrete type that is parameterized by another type.

Interestingly - the more detail we know about the type, the more things we can do with it inside our function - but the tradeoff then is that we can have less certainty about what the function is doing.

The cannonical example of this is something called the Identity function
```

---

# Going Fully Polymorphic

```haskell
identity :: a -> a
```

```notes
Let's assume for this - we're in a statically typed pure functional language. This function has to be pure.

We can't event do reflection (which is impure, by the way)

The identity function takes a single argument. We have no constraint on the type.

It could be anything. A class, a string, a number, a list of numbers, etc.

Are there any assertions we can safely make about it given only the type signature?

* Don't have the constructor. Can't create a new one - can't peform any operations on it.
* Can't *not* return it
* It has to be pure, so it cannot throw exceptions
* It can't return null

The only possible meaningful implementation is that the argument passed in is returned.

I say meaningful because you could declare some local variable within it and then do nothing with it. But it is still referentially transparent. If you swapped one version that doesn't declare a local variable for one that does, it would have no meaningful impact on the output of your program. It would have a little more overhead in terms of memory/cpu, but it doesn't change the correctness of the program.
```

---

# Going Fully Concrete

```haskell
foo :: Int -> Int
```

```notes
What do we know about this function?

* Could pass the value straight through?
* Could just return a constant?
* Could add 5?
* Could add 6?
* Could multiply by 5?
* Could multiply by 6?

The list of possible implementations is practically infinite

There is a cost to being specific. Concreteness has a cost.
* The less we know about a type - the more we know about what it does.
* The more we know about a type - the less we know about what it does.

There is an inverse relationship here.
```

---

# Quiz time!

---

```haskell
bar :: a -> b -> a
```

```notes
What do we know about this function?

It can only give back the a

This is called const. This is actually useful. It sounds crazy. I'm not even sure I understand the ways in which this IS useful, but when you have an ecosystem of these smaller composable functions, you find some interesting applications for them.
```

---

```haskell
baz :: a -> a -> a
```

```notes
What is possible here?

The specific implementation will need to pick which one it wants to always return, and it will always return that one.
```

---

```haskell
quux :: [a] -> a
```

```notes
What if the list is empty? Implicit in the list type is the ability to be empty.

This is illegal. Will not compile.

Cannot construct an a - cannot return null
```

---

```haskell
derp :: [a] -> Maybe a
```

```notes
This is the same type signature as head

What do we know about this?

* Still cannot construct new a's
* Could return Nothing every time

What do we know the list DIDN'T do to the a?

If we got an a back - it came from the list

The real power here is that just by looking at the type signature, we can make assertions about what the function does.

This is why purity and immutability are requirements. If we threw those out, we could NOT make these same assertions. It is also why there is such an intense focus on type signatures.

As we continue down the road of FP, we'll continue to be looking very closely at type signatures.
```

---

```haskell
fum :: [a] -> [a]
```

```notes
Could give you a list that is double the size?
Could just return an empty list?
- Items in the resulting list came from the input list
```

---

## Multiple Parameterized Types

```haskell
Map a b -- Dictionaries are called Maps in haskell
(a, b)
```

```notes
Data structures can be parameterized by more than one type.

Dictionaries could have different types for their keys and values

You've also got tuples
```

---

```haskell
zot :: Map a b -> Map a b
```

```notes
Cannot invert the keys and values
Could return an empty dictionary
Key value pairs in the resulting dictionary came from the input
```

---

## Recap

- Functions with a polymorphic type
- Functions with a concrete type parameterized by a polymorphic type
- Functions with a concrete type parameterized by more than one polymorphic type

---

## Higher Kinded Types

Function with a polymorphic type parameterized by a polymorphic type

---

Instead of

```haskell
List a
```

We have

```haskell
f a
```

```notes
This is where intuitions can start to break down. Most languages don't have the capability to express this. Some that do are haskell, purescript, scala

So, the f here could be any type that is parameterized by another type.

It could be a list of a, but it could also be a maybe of a. It could be a Future of a (like a promise)
```

---

```haskell
wibble :: f a -> f a
```

```notes
We're back at identity

This is where constraints REALLY come in.

If you put constraints on the a with a List of a we can start to do things like combine them in some way and give back an a, and that's useful!

If we constrain the f though - we can start to do some really interesting things. What if we knew that f had a mystery function with the following interface?
```

---

```haskell
mystery :: HasOurMysteryThing f => (a -> b) -> f a -> f b
```

```notes
Let's take a look at this type signature and see if we can figure out what function this matches.

We have a (a -> b) function as our first parameter, an f of a as our second, and we will produce an f of b.

Keep in mind that f here is alredy constrained to be able to take part in this operation. If there was no constraint at all, we couldn't change anything.

Can anyone guess?

Given that we know nothing about a or b, we can probably guess that we're going to use the (a -> b) function to turn our a into a b, right?
```

---

```haskell
fmap :: Functor f => (a -> b) -> f a -> f b
```

```notes
This is map! If we have a generic data structure that implements map we have a "functor". And each functor has to have its own implementation of map.

List of course will apply it to each item in the list - maybe MIGHT apply the function if the Maybe actually contains a value. If it doesn't, it won't run it.

What can we assert about the resulting f of b value here?

It *WAS* produced by running our (a -> b) function on the f of a ONCE.

There is no other possible implementation.

If you look at map's signature - that function knows nothing about a or b. It only knows a LITTLE bit about the f (only that it implements map).

When we make our functions highly polymorphic, they have to do exactly what they say which is often very constrained. They can't deviate from that, and thus we can be sure they will be usable down the road.
```

---

> "The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be absolutely precise."

— Edsger W. Dijkstra

```notes
Tying this all together is this quote from Edsger Dijkstra

People hear abstraction and think 14 layers of inheritance. That is not abstraction. It doesn't help you know more about the thing you're talking about.

Abstraction is about stripping away everything that is not relevant to the concept being conveyed. When you do inheritance in a traditional OO sense, you are almost exclusively adding more at each level.
```
